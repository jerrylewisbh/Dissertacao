\chapter{\hspace*{3pt} Fundamental Concepts}
\label{chapter:related-concepts}


This chapter introduces the theoretical concepts used to contextualize the applied approach for classifying documents on the Web.
The concepts related to Information Retrieval (\gls{ir}) and Documents Representation are essential to understanding the tasks applied in the proposed solution to extract the entities and the categories from text-resources online.
An overview of the concept and methods for the automatic classification of textual resources, the primary goal of the research presented in this thesis, is presented below. 


%\section{\hspace*{3pt}Natural Language Processing - NLP}

%\gls{nlp} consists of the development of mathematical and computational models of various aspects of language, to make possible the development of a wide range of systems that rely on information expressed in natural language \cite{joshi1991natural}.

%Natural language is any language developed naturally by the human being, in an unpremeditated way, as a result of the natural ease of language processing by the human intellect \cite{chomsky1975logical}. Spoken, written and sign languages are examples of natural languages. 

%\gls{nlp} has the general objective of processing human language so that it is understandable by the computer. Among the essential areas of \gls{nlp} we can highlight the recognition of voice, the recognition of writing and the reproduction of voice from the text. \gls{nlp} encompasses several and complex areas of knowledge, hence the  researches in this field are interdisciplinary, involving concepts of computer science, linguistics, logic, and psychology.

%One of the uses of \gls{nlp} is the extraction of meaning from unstructured resources so that they can be used in a structured way in computational application.


\section{\hspace{3pt}Information Retrieval}

\gls{ir} systems are mostly known for their searching ability, where a user states an information need and the system provides the user with a response to this information need in return. \gls{ir} is a large academic field and encompasses several topics such as browsing or filtering documents, processing of retrieved documents and clustering or classifying documents according to their content. In \cite{Manning:2008}, Manning defines \gls{ir} as finding material (usually documents) of an unstructured or semi-structured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).

Structured resources are machine-readable resources that encode relationships of various types according to the level of information \cite{hovy2013collaboratively}. They are of high quality as they are built from the knowledge of domain experts, lexicographers, and linguists, but limited because they require significant efforts in creation and updating. Because they are built manually, they depend on the availability of experts to extend their coverage and to keep them up to date on recent events. Moreover, knowledge encoded in one language is not transferable to others, requiring a new effort for each new language.

The most common structured resources are:

\begin{itemize}

\item Thesaurus: collections of related terms;
\item Taxonomies: Hierarchical structures of classification of terms;
\item Ontologies: knowledge models that include concepts, relations of different types, rules and axioms.
\end{itemize}

Unstructured resources are collections of texts that have no formalized knowledge and are machine readable only as sequences of characters and words. Different statistical models can extract knowledge from unstructured collections, and the vast number of texts available on the \gls{www} enables the construction of knowledge bases with extensive coverage. However, they are limited by the lack of texts that demonstrate common-sense knowledge \cite{hovy2013collaboratively}. Also, statistical models are not able to issue knowledge with quality equivalent to the resources built by specialists.

The limitations of unstructured resources are complementary to those of structured resources. While unstructured resources enable broad coverage with low quality, structured resources have high quality but low coverage. Semi-structured resources constructed collaboratively on the \gls{www} encode the knowledge voluntarily made available by users of these resources, covering different areas of expertise and having quality comparable to that obtained from specialists. Examples of semi-structured resources are Wiktionary, Flickr, Twitter, Yahoo! Answers and, Wikipedia \cite{hovy2013collaboratively}.

Nowadays, most of government, industry, business, and other institutions information are stored electronically on the Web as semi-structured or e-structured resources.

In this context, the existence of \gls{ir} systems becomes indispensable in assisting users in the process of locating relevant information in collections of unstructured or semi-structured data (e.g., web pages, documents, images, videos).  


A system can increase its precision when addressing the users' information need if it indexes well-represented features of the text. As the collection of documents expands, automatic techniques that can extract these features become crucial. Text classification techniques can provide an representative output of a research document, allowing for \gls{ir} systems to handle the indexing and retrieval process.


In order to reduce the complexity of the documents and make them easier to handle for \gls{ir} systems, each document has to be transformed from the full-text version to a compact representation, which describes its contents \cite{meadow1992text}. This task of document representation is crucial for text classification approaches. 

\subsection{\hspace*{3pt}Documents Representation}

%Document representation has substantial importance when dealing with text classification.

Plain texts are usually not used directly by classification algorithms.  The documents are processed and transformed to represent the semantic content of the text, optimized for numeric processing.


The \gls{vsm} is a simple, traditional and practical model that makes it possible to represent documents as vectors and to perform any algebraic operations to compare them ~\cite{salton1988term}.

In this method, the documents of a collection $D$ are represented in the \gls{vsm} as points in a multidimensional Euclidean space, where each dimension corresponds to a distinct term in that collection. The set $T$ of distinct terms of collection $D$, called vocabulary collection of $D$, is obtained in a process called lexical analysis.


This type of representation is widely used in \gls{ir}, in tasks of textual retrieval, ordering of documents by relevance (ranking), and text classification tasks. The use of vector representation makes the use of any algebraic operation applicable to this type of structure possible, enabling comparisons 
%of the similarities 
between two 
documents, as explained by Salton \cite{Salton:1975}.

Each term of the set $T$ can be composed of only one word (unigrams), several words (bigrams, trigrams or n-grams) or sentences, and has an associated weight to determine its degree of importance ~\cite{salton1988term}.

Given a document $d_i \in D$, this document is formally represented in the \gls{vsm} as follows $d={w_{i1},w_{i2},w_{i3},\ldots,w_{i|T|}}$, where $T$ is the vocabulary set of the collection $D$ and $w_{ij} (1 \le j \le |T|)$ is the weight of the term $t_j$ in the document $d_i$, such that $w_{ij} = 0$ if the term $t_j$ does not occur in the document $d_i$.

To represent the text in the \gls{vsm}, most text classification methods use the \gls{bow} approach to represent documents \cite{Lan:2009} \cite{Manning:2008}. The categorization takes into account the presence or the absence of key terms in the document-terms matrix %representing each document
\cite{Sebastiani:2002}.

The reasons for using this approach is the simplicity, efficiency and relative effectiveness of the \gls{bow} paradigm. However, the \gls{bow} method fails to take into account relevant aspects of the text that is being represented. Semantic relationships between key terms are ignored, as well as the order in which the terms appear \cite{Gabrilovich:2005, Lan:2009}. The \gls{bow} approach ignores essential semantic relations between the terms \cite {Hu:2008}.

Elements of bipartite words such as ``White House" or ``Bill Gates" are represented in the BOW as unrelated words. In analyzing the BOW representation of a given document in which the words ``bill" and ``gates" occur, one might suggest that the document talks about accounting for a construction firm (the word ``bill" for accounting, and construction from ``gates"). For a computer program, it would be challenging to associate these words.
Nevertheless, if the representation of the same document contains the set of words ``Bill Gates" as a term, it would be easier for the classifier to make a correct association. \cite{bekkerman2004using}.


As a consequence, if two documents use different sets of keywords to describe the same topic, they can be classified as being of different categories, even if the keywords used by both are synonymous or semantically associated in some other form \cite {Hu:2009}.

Among the alternative representations that use features of the text itself, most common are those that use sequential co-occurrence of n terms (n-grams) and non-sequential co-occurrences of n terms (term sets). Other approaches have explored features that are not directly extracted from the text. The growing interest in \gls{fg} techniques known as Document Expansion or Document Enrichment, through which new terms are added to documents, enhancing the \gls{bow} representation by inserting more information in the document-term matrix \cite {Gabrilovich:2005} is an example of this. Numerous methods that use \gls{fg} have achieved verifiable results in text classification through the extraction of semantic relations such as synonymy, hyponymy and associative relations between concepts, present in encyclopedias, thesauri, ontologies, web pages and other sources \cite{Gabrilovich:2005,Gabrilovich:2006,Hu:2008,Wang:2008, Wang:2009}.

Since many texts used in the Web (search queries, tweets, questions, and answers) are short, unstructured and ambiguous, there is a need for a methodology capable of analyzing short text semantically, detecting possible entities present in the sentence, disambiguating between terms, and overcoming the gaps of tradition methods. The approach described in this thesis employs the named entities found in the text as the basis for the representation, enabling the capture of semantic information related to the documents present on the Web.


\subsection{\hspace*{3pt}Named Entity Recognition}

Grishman and  Sundheim \cite{grishman1996message} defined the task of \gls{ner}, as one ``which involves identifying the names of all people, organizations and geographic locations in a text", whilst also involving the identification of date expressions, %hour
time, monetary values and percentages. 

The \gls{ner} task has been researched under several names over the years, for example Wikification \cite{Ratinov:2011}, Grounding \cite{leidner2003grounding} or Named-entity disambiguation \cite{hoffart2011robust}. The common approach can be generalized into the following steps: finding named entity mentions in a given text; generate a set of candidates for each mention; select the best candidate; and link the selected mention to the corresponding entry in the knowledge base.

In its most common form, the \gls{ner} task recognizes a predefined number of semantic categories, such as those defined in \cite{grishman1996message}. However, it has also been successfully applied to specific domains such as biology \cite{campos2012biomedical} and geology \cite{sobhana2010conditional}, where a more substantial number of domain-related categories are used. Another prevalent form is the use of linguistic rules for the recognition of entities. In this approach, the rules are manually coded from grammatical and domain knowledge, requiring specialization in both to obtain good results \cite{nadeau2007survey}. The use of linguistic rules restricts its applicability to documents written in the language for which the rules were codified, making it unusable with other languages.

The \gls{ner} task consists of two phases: i) the annotation of the grammatical classes of the text, and ii) the annotation of the names with the semantic category.

The annotations identify the grammatical class for each word in the text. If the word is an entity name, they identify its type. The quality of the algorithm is dependent on the annotator's ability to identify the names correctly, and it is limited to the types of entities used in the corpus. 

Figure \ref{fig:obama-example} shows an example of the \gls{ner} task in a post extracted from a social network\footnote{\url{https://twitter.com/BarackObama/status/925526988659548160}}.  The word ``Michelle" was identified as the Person Michelle Obama and the Words ``New York" were identified as the Place New York City. 

 
\begin{figure}[H]
  \includegraphics[width=\linewidth]{obama}
  \caption{Example of the \gls{ner} task on a text extracted from a social network.}
  \label{fig:obama-example}
\end{figure}


\section{\hspace*{3pt}Automatic Text Classification}
\label{sec:definition-automatic-text-classification}

Automatic text categorization is the activity of assigning text in natural language with categories from a predefined set according to their content. \cite{yang1999evaluation}. 

This task can be formalized as follows: let $D = \{d_1,d_2,d_3,\ldots\,d_n\}$ be a finite set of documents and $C = \{c_1,c_2,c_3,\ldots\,c_n\}$ a finite set of predefined categories.

The problem is finding a function $f: D \times C \mapsto \mathbb{R}$ that assigns a score $s$ for each pair 
$\{d_i, c_i\} \in  D \times C $, where the membership value of  $s$ specifies the degree of relevance of the category $c_i$ to the document $d_j$

Two different types of text categorization task can be identified depending on
the number of categories that could be assigned to each document. The first
type, in which precisely one category is assigned to each $d_j \in D$, is named as the single-class (or non-overlapping categories) text categorization task.
The second type, in which any number of categories from zero to $|C|$ may be
assigned to each $d_j \in D$, is called the multi-class (or overlapping categories) task.


According to Sebastiani\cite{Sebastiani:2002}, the definition of a text classifier can be summarized in the following steps:

\begin{enumerate}
\item  Acquisition of documents belonging to the domain (i.e.,
a collection of documents in $D$);
\item Creation of a vocabulary of $T$ terms  $\{w_1,w_2,w_3,\ldots,w_T\}$ that will be used in the representation of documents. This step involves preprocessing, such as lexical analysis (removal of digits, punctuation marks), removal of stopwords (articles, prepositions), and the stemming of words (reduction of the word to its radical) among other text operations;

\item Creation of the initial representation of documents with the definition of the set of attributes that describe them. Each attribute may be merely a Boolean value that indicates whether or not a given vocabulary term exists in the document (i.e., Boolean representation).  Each attribute can also correspond to a numerical weight associated with a given term, indicating its relevance to the document being described. 

\item Dimensionality Reduction, where the M most relevant attributes of the initial representation are selected (with M <T). This step can be done using different criteria for the selection of attributes, such as Information Gain, Mutual Information, $\tilde{\chi}^2$ statistic, and others \cite{Yang:1997}.

\item Induction of the classifier from a training set. %At section \ref{sec:text-classifiers}, we will address some algorithms that can be used to generate classifiers;%

\item Performance evaluation from a test set. Metrics used to evaluate classifiers include precision, accuracy,  recall, and F-measure. Further details regarding these evaluation metrics can be found in the description of the experiments (chapter \ref{chapter:experiments}).

\end{enumerate}


The methods that are used in text classification are frequently the same as those used in the more general area of \gls{ir}, where the goal is to find documents or sections within documents that are related to a particular query. Text classification methods are essential to finding relevant information in many different tasks that deal with large volumes of text-based information, such as finding Internet pages on a given subject, finding answers to similar questions that have been answered before, or classifying news by subject or newsgroup, among others. In each case, the goal is to assign the appropriate category or label to each document that needs to be classified.

Over the last few years, a vast number of algorithms have been proposed for text classification using machine learning. Among them, one can cite the naive Bayes \cite{mccallum1998comparison}, \gls{knn} \cite{Rijsbergen:1979}, \gls{svm} \cite{joachims1998text} and rule learning algorithms \cite{slattery1998combining}. %, which have been widely used. 
Research focusing on document classification usually reports on performance comparisons between different available algorithms.

The approach proposed in this thesis is a multi-label classification method, capable of assigning different degrees of membership for a document regarding each one of the categories available. Multi-label classification problems can usually be reduced to a particular case of single-label classification. To make the results suitable for comparison with other approaches and with the judgment of humans, multi-label classifications were converted to a single-label assignment by considering only the categories with the highest degrees of membership. 
%during the experiments we transformed 

